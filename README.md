# context_distillation
LLM Finetuning Project - Implementation of Context Distillation as described in https://arxiv.org/abs/2209.15189

- Using "Teacher_model_context_distillation" for extraction of of teacher probability distribution and evaluating baseline performances

- "Context_Distillation_Final" notebook provides code for finetuning and the generated outputs of the finetuned student model
